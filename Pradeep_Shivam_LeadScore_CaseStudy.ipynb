{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7c32ff4",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "An education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. \n",
    "\n",
    "The company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. \n",
    "\n",
    "Now, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as ‘Hot Leads’. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone.\n",
    "\n",
    "As you can see, there are a lot of leads generated in the initial stage (top) but only a few of them come out as paying customers from the bottom. In the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion.\n",
    "\n",
    "X Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with a higher lead score have a higher conversion chance and the customers with a lower lead score have a lower conversion chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ef5108",
   "metadata": {},
   "source": [
    "### Step 1: Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e39c4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppressing Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d2d0a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Pandas and NumPy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b760366e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Leads.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importing and inspecting the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m leads_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeads.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m leads_data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Leads.csv'"
     ]
    }
   ],
   "source": [
    "# Importing and inspecting the dataset\n",
    "leads_data = pd.read_csv(\"Leads.csv\")\n",
    "leads_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the dimensions of the dataframe\n",
    "leads_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef1abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the statistical aspects of the dataframe\n",
    "leads_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36fcbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the type of each column\n",
    "leads_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of unique values in all the cols. If a column has 90% or more the one single value we delete that column.\n",
    "for cols in leads_data.columns:\n",
    "    print(f\"column name : {cols} :  no of unique values --> {leads_data[cols].nunique()}\\n\")\n",
    "    print(leads_data[cols].value_counts().sort_index())\n",
    "    print(\"---------------------------------------------\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d779234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing columns of no significant importance\n",
    "leads_data.drop(columns=['Prospect ID','Do Not Email','Do Not Call','What matters most to you in choosing a course',\n",
    "                                    'Search','Magazine','Newspaper Article','X Education Forums','Newspaper','Digital Advertisement',\n",
    "                                   'Through Recommendations','Receive More Updates About Our Courses','Update me on Supply Chain Content',\n",
    "                                    'Get updates on DM Content','I agree to pay the amount through cheque'],inplace=True,axis=1)\n",
    "\n",
    "# we delete the 'Prospect ID' column also since it is of no use in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee0bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the value 'Select' as a missing value in cols\n",
    "leads_data.City.replace('Select',np.nan,inplace=True)\n",
    "leads_data.Specialization.replace('Select',np.nan,inplace=True)\n",
    "leads_data['How did you hear about X Education'].replace('Select',np.nan,inplace=True)\n",
    "leads_data['Lead Profile'].replace('Select',np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b141b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating percentage of null values in all the remaining cols\n",
    "(100*leads_data.isnull().mean()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fedc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with missing values more than or equal to 35%\n",
    "leads_data.drop(columns=['How did you hear about X Education','Lead Profile','Lead Quality','Asymmetrique Profile Score','Asymmetrique Profile Index',\n",
    "                         'Asymmetrique Activity Index','Asymmetrique Activity Score','City','Tags','Specialization'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38254efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values in columns 'What is your current occupation','Country' should be replaced by the mean, median or \n",
    "# mode value whichever applicable\n",
    "leads_data[['What is your current occupation','Country']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85dc830",
   "metadata": {},
   "source": [
    "Since both the above columns are of object type , we need to replace the missing values in these columns by their respective mode value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02816c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_wid_missingvalues=['What is your current occupation','Country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb19a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the missing values with modes of the respective cols\n",
    "for i in cols_wid_missingvalues:\n",
    "    leads_data[i].fillna(leads_data[i].mode()[0],axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df01c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows in columns where missing value percentage is too low\n",
    "leads_data.dropna(subset=['Page Views Per Visit','TotalVisits','Last Activity','Lead Source'],axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a23b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing values again in cols\n",
    "100*(leads_data.isnull().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd84f9e0",
   "metadata": {},
   "source": [
    "Now there are no missing values in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5269fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shape of the dataframe\n",
    "leads_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c2e84",
   "metadata": {},
   "source": [
    "#### Converting the binary variables (Yes/No) to 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb6eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying a binary map function\n",
    "leads_data['A free copy of Mastering The Interview'] = leads_data['A free copy of Mastering The Interview'].map({'Yes': 1, \"No\": 0}).astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(leads_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the column names\n",
    "leads_data.rename(columns={'Total Time Spent on Website':'Web_Time','Page Views Per Visit':'Page_Views','What is your current occupation':'Occupation',\n",
    "                           'A free copy of Mastering The Interview':'Interview_Copy','Last Notable Activity':'Last_Notable_Act'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping some more insignificant columns\n",
    "leads_data.drop(columns=['Last_Notable_Act'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989eea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_data=leads_data[['Lead Number','Lead Origin', 'Lead Source', 'TotalVisits', 'Web_Time',\n",
    "       'Page_Views', 'Last Activity', 'Country', 'Occupation',\n",
    "       'Interview_Copy','Converted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb7637e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leads_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1857bc8f",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ddfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns\n",
    "# Set style\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# See distribution of each of these columns\n",
    "fig = plt.figure(figsize = (14, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(leads_data.TotalVisits, bins = 80)\n",
    "plt.title('Total website visits')\n",
    "plt.savefig(f'D:/UPGRAD/lead scoring images/Web_visits.png',dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(leads_data.Web_Time, bins = 30)\n",
    "plt.title('Time spent on website')\n",
    "plt.savefig(f'D:/UPGRAD/lead scoring images/Web_Time.png',dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(leads_data.Page_Views, bins = 80)\n",
    "plt.title('Average number of page views per visit')\n",
    "plt.savefig(f'D:/UPGRAD/lead scoring images/Page_Views.png',dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7efaa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols=leads_data.select_dtypes(include='number').columns\n",
    "cat_cols=leads_data.select_dtypes(include='object').columns\n",
    "print(f'numeric columns\\n{num_cols}')\n",
    "print()\n",
    "print(f'categorical columns\\n{cat_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67191771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through categorical columns and create separate plots\n",
    "for col in cat_cols:\n",
    "    plt.figure(figsize=(7, 5))  # Create a new figure for each plot\n",
    "    \n",
    "    # Countplot for each categorical variable\n",
    "    sns.countplot(data=leads_data, x=col, palette=\"viridis\")\n",
    "    \n",
    "    # Formatting\n",
    "    plt.title(f'Count of {col}', fontsize=14)  \n",
    "    plt.xlabel(col, fontsize=12)             \n",
    "    plt.ylabel(\"Count\", fontsize=12)           \n",
    "    plt.xticks(rotation=90, fontsize=10)       \n",
    "\n",
    "    plt.tight_layout()  # Prevent overlapping layout\n",
    "    plt.savefig(f'D:/UPGRAD/lead scoring images/{col}.png',dpi=300, bbox_inches='tight')\n",
    "    plt.show()  # Show each plot separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae12428f",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "1. **Country**: Most leads come from **India**, suggesting a focus on Indian audiences with potential expansion elsewhere.\n",
    "\n",
    "2. **Interview_Copy**: Those who request the free interview guide show **higher intent**, making them prime targets for personalized follow-ups.\n",
    "\n",
    "3. **Last Activity**: Website visits and email opens are the most common final actions, indicating the importance of **website content** and **email campaigns**.\n",
    "\n",
    "4. **Lead Origin**: **Landing Page Submissions** generate the most leads, so optimizing landing pages can further boost conversions.\n",
    "\n",
    "5. **Lead Source**: **Olark Chat** and **Organic Search** are top channels, emphasizing the need for strong **live chat support** and **SEO** strategies.\n",
    "\n",
    "6. **Occupation**: The majority of leads are **Unemployed**, highlighting the importance of promoting **career-building** benefits to this group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5891d",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0932f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# visualising the correlations via pairplot for numeric variables\n",
    "sns.pairplot(leads_data[['TotalVisits','Web_Time','Page_Views','Interview_Copy','Converted']],diag_kind='kde',hue='Converted')\n",
    "# Save the figure in a folder\n",
    "plt.savefig(\"D:/UPGRAD/lead scoring images/PairplotforNumVars.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59380566",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "1. **High Website Engagement → Better Conversions**: More time on site and more page views often lead to higher conversion rates.  \n",
    "2. **Free Copy Request Signals Intent**: Leads who request “Mastering the Interview” show higher intent and are more likely to convert.  \n",
    "3. **Web Time & Page Views Correlate**: Visitors who spend more time also view more pages, indicating deeper engagement.  \n",
    "4. **Total Visits ≠ Guaranteed Conversion**: Frequent visits alone don’t ensure conversion; the quality of engagement matters more.  \n",
    "5. **Prioritize High-Engagement Leads**: Focus on leads who show multiple signs of interest (e.g., free copy request, long site visits) for targeted follow-ups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810227d",
   "metadata": {},
   "source": [
    "##### For categorical variables with multiple levels, creating dummy features (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a62dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables for the categorical variables and dropping the first one.\n",
    "dummy1 = pd.get_dummies(leads_data[['Lead Origin', 'Lead Source', 'Last Activity', 'Country','Occupation','Interview_Copy']], drop_first=True,dtype=int)\n",
    "\n",
    "# Adding the results to the master dataframe\n",
    "leads_data = pd.concat([leads_data, dummy1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1470a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d894b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the original columns for which dummy variables have been created\n",
    "leads_data.drop(columns=['Lead Origin', 'Lead Source', 'Last Activity', 'Country','Occupation','Interview_Copy'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdfb9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742fa2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating percentage of null values in all the cols\n",
    "(100*leads_data.isnull().mean()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34cc13",
   "metadata": {},
   "source": [
    "#### Checking outliers for the continuous numeric variables except the dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22258757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for outliers in the numeric variables\n",
    "num_leads = leads_data[['TotalVisits','Web_Time','Page_Views']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%\n",
    "num_leads.describe(percentiles=[.25, .5, .75, .90, .95, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25155ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing for outliers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(num_leads)\n",
    "plt.savefig('D:/UPGRAD/lead scoring images/num_outlier.png',dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1807d20",
   "metadata": {},
   "source": [
    "We can see that the columns TotalVisits and Page_Views have outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab14d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating percentage of null values in all the cols\n",
    "(100*leads_data.isnull().mean()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f756f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check for outliers\n",
    "cols = ['TotalVisits', 'Web_Time', 'Page_Views']\n",
    "\n",
    "# Start with a boolean mask that is True for all rows\n",
    "mask = pd.Series(True, index=leads_data.index)\n",
    "\n",
    "# Loop through each column and update the mask:\n",
    "# Only keep rows where the value is between the 1st and 99th percentile.\n",
    "for col in cols:\n",
    "    # Calculate the 1st and 99th percentiles for the current column\n",
    "    lower_bound = leads_data[col].quantile(0.01)\n",
    "    upper_bound = leads_data[col].quantile(0.99)\n",
    "    print(f\"For column '{col}': Lower bound = {lower_bound}, Upper bound = {upper_bound}\")\n",
    "    \n",
    "    # Update the mask: a row remains True only if its value in this column is within bounds.\n",
    "    mask &= (leads_data[col] >= lower_bound) & (leads_data[col] <= upper_bound)\n",
    "\n",
    "# Apply the mask to the entire DataFrame.\n",
    "# This removes any row that has an outlier in any of the three columns.\n",
    "leads_data = leads_data[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"New shape of leads_data after outlier removal:\", leads_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_data[['TotalVisits','Web_Time','Page_Views']].describe(percentiles=[0.25,0.5,0.75,0.90,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b972020",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(leads_data[['TotalVisits','Web_Time','Page_Views']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804cf8a6",
   "metadata": {},
   "source": [
    "Now we can see that there are negligible number of outliers remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a8521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leads_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b38154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leads_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb74437",
   "metadata": {},
   "source": [
    "We can see that all the variables are of numeric type now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b20a8",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa97e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b1f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting feature variable to X\n",
    "data = leads_data.copy()\n",
    "\n",
    "X = leads_data.drop(['Converted'], axis=1)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting response variable to y\n",
    "y = leads_data.Converted\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e012f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_number_train = X_train['Lead Number']\n",
    "lead_number_test  = X_test['Lead Number']\n",
    "\n",
    "# Now drop the Lead Number column from the feature sets that go into modeling.\n",
    "X_train_model = X_train.drop('Lead Number', axis=1)\n",
    "X_test_model  = X_test.drop('Lead Number', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d4f7df",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e96b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab96f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_model[['TotalVisits','Web_Time','Page_Views']] = scaler.fit_transform(X_train_model[['TotalVisits','Web_Time','Page_Views']])\n",
    "\n",
    "X_train_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e68ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking the Lead Conversion Rate\n",
    "(sum(leads_data['Converted'])/leads_data.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63fe0f",
   "metadata": {},
   "source": [
    "The conversion rate is about 37% which indicates that there is no class imbalance in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be09e895",
   "metadata": {},
   "source": [
    "#### Looking at Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the correlation matrix \n",
    "plt.figure(figsize = (80,80))        # Size of the figure\n",
    "sns.heatmap(X_train.corr(),annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93110df3",
   "metadata": {},
   "source": [
    "### Dropping highly correlated dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3dd7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model = X_test_model.drop(['Occupation_Working Professional'], axis=1)\n",
    "X_train_model = X_train_model.drop(['Occupation_Working Professional'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10684118",
   "metadata": {},
   "source": [
    "#### Model Building and Automated Feature Selection Using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd396bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba1a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(logreg, n_features_to_select=15)             # running RFE with 15 variables as output\n",
    "rfe = rfe.fit(X_train_model, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b325d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f09470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert np.bool_ and np.int64 to native Python types\n",
    "output = [(col, bool(s), int(r)) for col, s, r in zip(X_train_model.columns, rfe.support_, rfe.ranking_)]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5704c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 best features selected by the recursive feature elimination(RFE)\n",
    "col = X_train_model.columns[rfe.support_]\n",
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a5db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features not selected by rfe\n",
    "no_col=X_train_model.columns[~rfe.support_]\n",
    "print(no_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7165905e",
   "metadata": {},
   "source": [
    "##### Building and Assessing the model with StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building model using the 15 best features selected by rfe\n",
    "# Model 1\n",
    "X_train_sm = sm.add_constant(X_train_model[col])\n",
    "logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b9753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted values on the train set\n",
    "y_train_pred = res.predict(X_train_sm)\n",
    "y_train_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fec0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = y_train_pred.values.reshape(-1)\n",
    "y_train_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc49ca5",
   "metadata": {},
   "source": [
    "##### Creating a dataframe with the actual lead converted and the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2627a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Convert_Prob':y_train_pred})\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c10b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final['predicted'] = y_train_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Let's see the head\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f247e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544fd7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the confusion matrix in a presentable dataframe form\n",
    "confusion = metrics.confusion_matrix(y_train_pred_final['Converted'], y_train_pred_final['predicted'])\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating recall and accuracy score on the training data\n",
    "print('recall --> ', metrics.recall_score(y_train_pred_final['Converted'], y_train_pred_final['predicted']))\n",
    "print('accuracy --> ', metrics.accuracy_score(y_train_pred_final['Converted'], y_train_pred_final['predicted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10159e3d",
   "metadata": {},
   "source": [
    "The present recall score for the training set is almost 69%. We are supposed to maintain the recall score around 80%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374e752",
   "metadata": {},
   "source": [
    "#### Checking VIFs for multicollinearity among the independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the VIF values of the feature variables. \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e051ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2\n",
    "X_train_sm = sm.add_constant(X_train_model[col])\n",
    "lm = sm.GLM(y_train, X_train_sm,family = sm.families.Binomial()).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78edda07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking vif again for model 2\n",
    "df1 = X_train_model[col]\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = df1.columns\n",
    "vif['VIF'] = [variance_inflation_factor(df1.values, i) for i in range(df1.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'],2)\n",
    "print(vif.sort_values(by='VIF',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db654518",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop(['Lead Origin_Lead Add Form'],1) # since its vif is 98.39 >> 5\n",
    "print(f\"number of cols : {len(col)}\")\n",
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3\n",
    "X_train_sm = sm.add_constant(X_train_model[col])\n",
    "lm = sm.GLM(y_train, X_train_sm,family = sm.families.Binomial()).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking vifs again for model 3\n",
    "df1 = X_train_model[col]\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = df1.columns\n",
    "vif['VIF'] = [variance_inflation_factor(df1.values, i) for i in range(df1.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'],2)\n",
    "print(vif.sort_values(by='VIF',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop(['Country_Saudi Arabia'],1) # since its p value is more than 0.05\n",
    "print(f\"number of cols : {len(col)}\")\n",
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec73597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4\n",
    "X_train_sm = sm.add_constant(X_train_model[col])\n",
    "lm = sm.GLM(y_train, X_train_sm,family = sm.families.Binomial()).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking vifs again for model 4\n",
    "df1 = X_train_model[col]\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = df1.columns\n",
    "vif['VIF'] = [variance_inflation_factor(df1.values, i) for i in range(df1.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'],2)\n",
    "print(vif.sort_values(by='VIF',ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49265b4d",
   "metadata": {},
   "source": [
    "Now we can see that no p values are more than 0.05 and all the vifs are less than 3. So no need to drop any other feature now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb5da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final_new = pd.DataFrame({'Converted':y_train.values})\n",
    "y_train_pred_final_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f7fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the probabilities using the updated model\n",
    "y_train_pred_new = lm.predict(X_train_sm).values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd76a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final_new['Convert_Prob'] = y_train_pred_new\n",
    "y_train_pred_final_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new column 'predicted' with 1 if Converted_Prob > 0.5 else 0.\n",
    "# 0.5 being a random cutoff  probability threshold.\n",
    "y_train_pred_final_new['predicted'] = y_train_pred_final_new.Convert_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "y_train_pred_final_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0610040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the confusion matrix using the updated model(Model 4)\n",
    "cm=metrics.confusion_matrix(y_train_pred_final_new.Converted,y_train_pred_final_new.predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d34d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating final recall and accuracy score on the training data\n",
    "print('recall --> ', metrics.recall_score(y_train_pred_final_new['Converted'], y_train_pred_final_new['predicted']))\n",
    "print('accuracy --> ', metrics.accuracy_score(y_train_pred_final_new['Converted'], y_train_pred_final_new['predicted']))\n",
    "# Extract true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# Calculate specificity: TN / (TN + FP)\n",
    "specificity = TN / (TN + FP)\n",
    "print('specificity --> ', specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ec8a2",
   "metadata": {},
   "source": [
    "It seems that after deleting few features also, the recall almost remains the same. So we need to try changing the cut_off probablility keeping in mind the target recall score of around 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff29ed8",
   "metadata": {},
   "source": [
    "### Plotting the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fdd952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc(actual, probs):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(actual, probs, drop_intermediate=False)\n",
    "    auc_score = metrics.roc_auc_score(actual, probs)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (area = {auc_score:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Save the figure before showing it\n",
    "    plt.savefig('D:/UPGRAD/lead scoring images/roc_auc.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf534b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# roc curve for the updated model (Model 4)\n",
    "draw_roc(y_train_pred_final_new.Converted, y_train_pred_final_new.Convert_Prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60600c84",
   "metadata": {},
   "source": [
    "### Finding Optimal Cutoff Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc08c3d",
   "metadata": {},
   "source": [
    "Optimal cutoff probability is that prob where we get balanced accuracy,sensitivity and specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_train_pred_final_new[i]= y_train_pred_final_new.Convert_Prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating accuracy, sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n",
    "\n",
    "# TP = confusion[1,1] # true positive \n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_pred_final_new.Converted, y_train_pred_final_new[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71224df5",
   "metadata": {},
   "source": [
    "From the above it is clear that our previous cutoff of 0.5 was not optimal. Instead something around 0.3 would be a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2322ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
    "plt.savefig('D:/UPGRAD/lead scoring images/recall_accuracy_balance.png',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c1ba1f",
   "metadata": {},
   "source": [
    "so around 0.33 cutoff would be better. lets try the cutoff 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the cutoff to 0.33\n",
    "y_train_pred_final_new['final_predicted'] = y_train_pred_final_new.Convert_Prob.map( lambda x: 1 if x > 0.33 else 0)\n",
    "y_train_pred_final_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee23c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the confusion matrix for the updated cutoff \n",
    "cm1=metrics.confusion_matrix(y_train_pred_final_new.Converted,y_train_pred_final_new.final_predicted)\n",
    "print(cm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf11dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating final recall and accuracy score on the training data\n",
    "print('recall --> ', metrics.recall_score(y_train_pred_final_new['Converted'], y_train_pred_final_new['final_predicted']))\n",
    "print('accuracy --> ', metrics.accuracy_score(y_train_pred_final_new['Converted'], y_train_pred_final_new['final_predicted']))\n",
    "# Extract true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP)\n",
    "TN, FP, FN, TP = cm1.ravel()\n",
    "\n",
    "# Calculate specificity: TN / (TN + FP)\n",
    "specificity = TN / (TN + FP)\n",
    "print('specificity --> ', specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaf68c1",
   "metadata": {},
   "source": [
    "so from the above we can see that we have achieved the recall as 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a54fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = lm.predict(X_train_sm)\n",
    "\n",
    "# Create a DataFrame for the training set results, ensuring all series are of equal length\n",
    "train_results = pd.DataFrame({\n",
    "    'Lead Number': lead_number_train,                     # should have the same length as X_train_model\n",
    "    'Converted': y_train,                                 # actual conversion values from training set\n",
    "    'Convert_Prob': y_train_pred,                         # predicted probabilities from the model\n",
    "    'Final_Predicted': [1 if x > 0.33 else 0 for x in y_train_pred],  # applying cutoff 0.33\n",
    "    'Lead_Score': (y_train_pred * 100).round(0)           # calculating lead score and rounding off\n",
    "})\n",
    "\n",
    "print(train_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying recall(sensitivity), specificity and accuracy for train_results\n",
    "print(f'recall --> {metrics.recall_score(train_results.Converted,train_results.Final_Predicted)}')\n",
    "print(f'specificity --> {metrics.recall_score(train_results.Converted,train_results.Final_Predicted,pos_label=0)}')\n",
    "print(f'accuracy --> {metrics.accuracy_score(train_results.Converted,train_results.Final_Predicted)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719591cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb8709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train_pred_final_new['Converted'], y_train_pred_final_new['final_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train_pred_final_new['Converted'], y_train_pred_final_new['final_predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73de10",
   "metadata": {},
   "source": [
    "### Step 11: Making predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the numeric variables in testing data using standard scaler\n",
    "X_test_model[['TotalVisits','Web_Time','Page_Views']] = scaler.transform(X_test_model[['TotalVisits','Web_Time','Page_Views']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c721eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same 15 cols in testing selected by rfe\n",
    "X_test_model = X_test_model[col]\n",
    "X_test_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a constant column\n",
    "X_test_sm = sm.add_constant(X_test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c25f3f",
   "metadata": {},
   "source": [
    "Making predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163343c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the probabilities using the final model\n",
    "y_test_pred = lm.predict(X_test_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547fc391",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2856ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y_pred to a dataframe which is an array\n",
    "y_pred_1 = pd.DataFrame(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6325e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the head\n",
    "y_pred_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d268a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y_test to dataframe\n",
    "y_test_df = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204eee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending y_test_df and y_pred_1\n",
    "y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final.rename(columns={0:'Convert_Prob'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf72717",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final['final_predicted'] = y_pred_final.Convert_Prob.map(lambda x: 1 if x > 0.33 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4538836",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f75b5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\n",
    "confusion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating final recall and accuracy score on the testing data\n",
    "print('recall --> ', metrics.recall_score(y_pred_final.Converted, y_pred_final.final_predicted))\n",
    "print('accuracy --> ', metrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted))\n",
    "print('specificity --> ', metrics.recall_score(y_pred_final.Converted, y_pred_final.final_predicted,pos_label=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ecdf6",
   "metadata": {},
   "source": [
    "From the above we can see that the recall on the test data comes as 80.2% which is very close to the recall on the training data. This indicates the model is perfectly ok."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb53ef",
   "metadata": {},
   "source": [
    "Now assigning a lead score for every lead number in the dataset. We are multiplying the Convert_Prob column by 100 thus obtaining the required lead score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predicted probabilities using your logistic regression model\n",
    "y_test_pred = lm.predict(X_test_sm)\n",
    "\n",
    "# Create the test results DataFrame ensuring all series have matching lengths\n",
    "test_results = pd.DataFrame({\n",
    "    'Lead Number': lead_number_test,\n",
    "    'Converted': y_test,  # actual conversion status from test set\n",
    "    'Convert_Prob': y_test_pred,\n",
    "    'Final_Predicted': [1 if prob > 0.33 else 0 for prob in y_test_pred],\n",
    "    'Lead_Score': (y_test_pred * 100).round(0)\n",
    "})\n",
    "\n",
    "print(test_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49103c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.concat([train_results, test_results], axis=0).reset_index(drop=True)\n",
    "\n",
    "# 'final_results' now clearly shows each Lead Number with its corresponding lead score.\n",
    "print(final_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936316e",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "- Based on our analysis, 0.33 emerged as the optimal probability cutoff, balancing recall, precision, and accuracy for the lead conversion model. \n",
    "\n",
    "\n",
    "- At this threshold, leads receive a Lead Score of 33 or higher and are deemed “hot.” Our model demonstrates approximately\n",
    "  80% recall on both training and test data, indicating it effectively captures the majority of likely converters without\n",
    "  significantly inflating false positives.\n",
    "  \n",
    "\n",
    "- Consequently, we recommend prioritizing outreach to leads with scores ≥ 33, as they have a high likelihood of conversion.\n",
    "  However, this threshold can be revisited if business objectives change—such as aiming to reduce the number of calls (requiring\n",
    "  higher precision) or ensuring fewer missed opportunities (requiring higher recall). Regularly reviewing the model’s     performance and adjusting the cutoff as needed will help maintain alignment with organizational goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f2cb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
